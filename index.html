<!DOCTYPE html> 
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Medical Hallucination in Foundation Models and Their Impact on Healthcare">
  <meta name="keywords" content="Medical Hallucination, Hallucination Mitigation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Medical Hallucination in Foundation Models and Their Impact on Healthcare</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/bed-svgrepo-com.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Medical Hallucination in Foundation Models and Their Impact on Healthcare</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ybkim95.github.io/">Yubin Kim</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="https://sites.google.com/view/hyewon-jeong/">Hyewon Jeong</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Shan Chen</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="#">Shuyue Stella Li</a><sup>3</sup>,
            </span>
            <span class="author-block">
                <a href="#">Mingyu Lu</a><sup>3</sup>,
            </span>
            <span class="author-block">
                <a href="#">Kumail Alhamoud</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Jimin Mun</a><sup>4</sup>,
            </span>
            <span class="author-block">
                <a href="#">Cristina Grau</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Minseok Jung</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Rodrigo Gameiro</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Lizhou Fan</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="#">Eugene Park</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Tristan Lin</a><sup>8</sup>,
            </span>
            <span class="author-block">
                <a href="#">Joonsik Yoon</a><sup>5</sup>,
            </span>
            <span class="author-block">
                <a href="#">Wonjin Yoon</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="#">Maarten Sap</a><sup>4</sup>,
            </span>
            <span class="author-block">
                <a href="#">Yulia Tsvetkov</a><sup>3</sup>,
            </span>
            <span class="author-block">
                <a href="#">Paul Liang</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Xuhai Xu</a><sup>7</sup>,
            </span>
            <span class="author-block">
                <a href="#">Xin Liu</a><sup>6</sup>,
            </span>
            <span class="author-block">
                <a href="#">Daniel McDuff</a><sup>6</sup>,
            </span>
            <span class="author-block">
                <a href="#">Hyeonhoon Lee</a><sup>5</sup>,
            </span>
            <span class="author-block">
                <a href="#">Hae Won Park</a><sup>1</sup>,
            </span>
            <span class="author-block">
                <a href="#">Samir Tulebaev</a><sup>2</sup>,
            </span>
            <span class="author-block">
                <a href="#">Cynthia Breazeal</a><sup>1</sup>
            </span>          
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Massachusetts Institute of Technology</span>
            <span class="author-block"><sup>2</sup>Harvard Medical School</span>
            <span class="author-block"><sup>3</sup>University of Washington</span>
            <span class="author-block"><sup>4</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>5</sup>Seoul National University Hospital</span>
            <span class="author-block"><sup>6</sup>Google</span>
            <span class="author-block"><sup>7</sup>Columbia University</span>
            <span class="author-block"><sup>8</sup>Johns Hopkins University</span>
        </div>
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/mitmedialab/medical_hallucination/blob/main/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.medrxiv.org/content/10.1101/2025.02.28.25323115v1"
                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <img src="https://www.medrxiv.org/sites/all/themes/mjolnir/logo.png" 
                             alt="MedRxiv" 
                             style="height: 24px; width: 24px;">
                    </span>
                    <span>MedRxiv</span>
                </a>
            </span>            
              <span class="link-block">
                <a href="https://github.com/mitmedialab/medical_hallucination"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
            <div class="is-size-5 publication-news">
              <br>
              <p>
                ðŸŽ‰ <strong>News:</strong> [2025-03-03] ðŸŽ‰ðŸŽ‰ðŸŽ‰ Our preprint paper has been submitted to <strong>medRxiv</strong>!
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!--video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/mainfigure.png" type="picture">
      </video-->
      <div style="text-align: center;">
        <img src="./static/images/taxonomy.png" width="60%">
      </div>
      <h2 class="subtitle has-text-centered">
        Figure 1. A visual taxonomy of medical hallucinations in LLMs, organized into five main clusters.
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Foundation Models that are capable of processing and generating multi-modal
            data have transformed AIâ€™s role in medicine. However, a key limitation of their
            reliability is hallucination, where inaccurate or fabricated information can impact
            clinical decisions and patient safety. We define medical hallucination as any
            instance in which a model generates misleading medical content. This paper
            examines the unique characteristics, causes, and implications of medical hal
           lucinations, with a particular focus on how these errors manifest themselves
            in real-world clinical scenarios. Our contributions include (1) a taxonomy for
            understanding and addressing medical hallucinations, (2) benchmarking models
            using medical hallucination dataset and physician-annotated LLM responses to
            real medical cases, providing direct insight into the clinical impact of hallucina
           tions, and (3) a multi-national clinician survey on their experiences with medical
            hallucinations. Our results reveal that inference techniques such as Chain
           of-Thought (CoT) and Search Augmented Generation can effectively reduce
            hallucination rates. However, despite these improvements, non-trivial levels of
            hallucination persist. These findings underscore the ethical and practical imper
           ative for robust detection and mitigation strategies, establishing a foundation for
            regulatory policies that prioritize patient safety and maintain clinical integrity
            as AI becomes more integrated into healthcare. The feedback from clinicians
            highlights the urgent need for not only technical advances but also for clearer
            ethical and regulatory guidelines to ensure patient safety.           </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-3">Survey on Medical Hallucination among Healthcare Professionals</h2>
        <div class="content">
          <p>
            To investigate the perceptions and experiences of healthcare professionals and researchers regarding the use of AI / LLM tools, particularly regarding medical hallucinations, we conducted a survey aimed at individuals in the medical, research, and analytical fields (Figure 9). A total of 75 professionals participated, primarily holding MD and/or PhD degrees, representing a diverse range of disciplines. The survey was conducted over a 94-day period, from September 15, 2024, to December 18, 2024, confirming the significant adoption of AI/LLM tools across these fields. Respondents indicated varied levels of trust in these tools, and notably, a substantial proportion reported encountering medical hallucinationsâ€”factually incorrect yet plausible outputs with medical relevanceâ€”in tasks critical to their work, such as literature reviews and clinical decision-making. Participants described employing verification strategies like cross-referencing and colleague consultation to manage these inaccuracies.
          </p>
          <img src="./static/images/survey1.png" alt="Bias Figure AgentClinic">
        </div>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-fullwidth">
        <h2 class="title is-3">LLM Experiments on Medical Hallucination Benchmark</h2>
        <div class="content">
          <p>
            This result reveals that the recent models (e.g. o3-mini, deepseek-r1, and gemini-2.0-flash) typically start with high baseline hallucination resistance and tend to see moderate but consistent gains from a simple CoT, while previous models including medical-purpose LLMs often begin at low hallucination resistance yet can benefit from different approaches (e.g. Search, CoT, and System Prompt). Moreover, retrieval-augmented generation can be less effective if the model struggles to reconcile retrieved information with its internal knowledge.          </p>
          <img src="./static/images/exp1.png" alt="Bias Figure AgentClinic">
        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-fullwidth">
        <div class="content">
          <h2 class="title is-3">Human Physicians' Medical Hallucination Annotation</h2>
          <p>
            To rigorously evaluate the presence and nature of hallucinations in LLMs within the clinical domain, we employed a structured annotation process. We built upon established frameworks for hallucination and risk assessment, drawing specifically from the hallucination typology proposed by Hegselmann et al. (2024b) and the risk level framework from Asgari et al. (2024) (Figure 6) and used the New England Journal of Medicine (NEJM) Case Reports for LLM inferences.          </p>
          <img src="./static/images/annotation.png" alt="Bias Figure AgentClinic">
          <p>
            To qualitatively assess the LLMâ€™s clinical reasoning abilities, we designed three targeted tasks, each focusing on a crucial aspect of medical problem-solving: 1) chronological ordering of events, 2) lab data interpretation, and 3) differential diagnosis generation. These tasks were designed to mimic essential steps in clinical practice, from understanding the patientâ€™s history to formulating a diagnosis.
          <img src="./static/images/annotation2.png" alt="Bias Figure AgentClinic">
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article {Kim2025.02.28.25323115,
      author = {Kim, Yubin and Jeong, Hyewon and Chen, Shen and Li, Shuyue Stella and Lu, Mingyu and Alhamoud, Kumail and Mun, Jimin and Grau, Cristina and Jung, Minseok and Gameiro, Rodrigo R and Fan, Lizhou and Park, Eugene and Lin, Tristan and Yoon, Joonsik and Yoon, Wonjin and Sap, Maarten and Tsvetkov, Yulia and Liang, Paul Pu and Xu, Xuhai and Liu, Xin and McDuff, Daniel and Lee, Hyeonhoon and Park, Hae Won and Tulebaev, Samir R and Breazeal, Cynthia},
      title = {Medical Hallucination in Foundation Models and Their Impact on Healthcare},
      elocation-id = {2025.02.28.25323115},
      year = {2025},
      doi = {10.1101/2025.02.28.25323115},
      publisher = {Cold Spring Harbor Laboratory Press},
      abstract = {Foundation Models that are capable of processing and generating multi-modal data have transformed AI{\textquoteright}s role in medicine. However, a key limitation of their reliability is hallucination, where inaccurate or fabricated information can impact clinical decisions and patient safety. We define medical hallucination as any instance in which a model generates misleading medical content. This paper examines the unique characteristics, causes, and implications of medical hallucinations, with a particular focus on how these errors manifest themselves in real-world clinical scenarios. Our contributions include (1) a taxonomy for understanding and addressing medical hallucinations, (2) benchmarking models using medical hallucination dataset and physician-annotated LLM responses to real medical cases, providing direct insight into the clinical impact of hallucinations, and (3) a multi-national clinician survey on their experiences with medical hallucinations. Our results reveal that inference techniques such as Chain-of-Thought (CoT) and Search Augmented Generation can effectively reduce hallucination rates. However, despite these improvements, non-trivial levels of hallucination persist. These findings underscore the ethical and practical imperative for robust detection and mitigation strategies, establishing a foundation for regulatory policies that prioritize patient safety and maintain clinical integrity as AI becomes more integrated into healthcare. The feedback from clinicians highlights the urgent need for not only technical advances but also for clearer ethical and regulatory guidelines to ensure patient safety. A repository organizing the paper resources, summaries, and additional information is available at https://github.com/mitmedialab/medical_hallucination.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis study did not receive any funding.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:This study received an Institutional Review Board (IRB) exemption from MIT COUHES (Committee On the Use of Humans as Experimental Subjects) under exemption category 2 (Educational Testing, Surveys, Interviews, or Observation). The IRB determined that this research, involving surveys with professionals on their perceptions and experiences with AI/LLMs, posed minimal risk to participants and met the criteria for exemption.I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.YesMed-HALT is a publicly available dataset and NEJM Medical Records can be access after the sign-up.https://www.nejm.org/browse/nejm-article-category/clinical-cases?date=past5Yearshttps://github.com/medhalt/medhalt},
      URL = {https://www.medrxiv.org/content/early/2025/03/03/2025.02.28.25323115},
      eprint = {https://www.medrxiv.org/content/early/2025/03/03/2025.02.28.25323115.full.pdf},
      journal = {medRxiv}
}</code></pre>
  </div>
</section>

</body>
</html>
